{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.cuda.amp.autocast_mode import autocast\n",
    "from torch.cuda.amp.grad_scaler import GradScaler\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Mini3(nn.Module):\n",
    "\n",
    "    def __init__(self, ftrs):\n",
    "        super(Mini3, self).__init__()\n",
    "        self.dcnv = nn.Conv2d(ftrs, ftrs // 2, 1)\n",
    "        self.cnv = nn.Conv2d(ftrs // 2, ftrs // 2, 3, 1, 1, bias=False)\n",
    "        self.ucnv = nn.Conv2d(ftrs // 2, ftrs, 1)\n",
    "        self.nrm = nn.BatchNorm2d(ftrs // 2)\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.dcnv(x))\n",
    "        x = self.act(self.nrm(self.cnv(x)))\n",
    "        x = self.act(self.ucnv(x))\n",
    "        return x\n",
    "\n",
    "class ReStep(nn.Module):\n",
    "\n",
    "    def __init__(self, ftrs, poolks, final=False):\n",
    "        super(ReStep, self).__init__()\n",
    "        self.cnv1 = Mini3(ftrs)\n",
    "        self.cnv2 = Mini3(ftrs)\n",
    "        self.cnv3 = Mini3(ftrs)\n",
    "        self.dscale = nn.Conv2d(ftrs, ftrs, 1, poolks, bias=False)\n",
    "        if final:\n",
    "            self.uscale = nn.Conv2d(ftrs, ftrs, 1, 1)\n",
    "        else:\n",
    "            self.uscale = nn.Conv2d(ftrs, ftrs * 2, 1, 1)\n",
    "        self.inrm = nn.BatchNorm2d(ftrs)\n",
    "        self.mxpool = nn.MaxPool2d(poolks, poolks)\n",
    "        self.iact = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.cnv1(x)\n",
    "        x = self.cnv2(x)\n",
    "        x = self.mxpool(x)\n",
    "        x = self.iact(self.inrm(torch.add(x, self.dscale(res))))\n",
    "        res = x\n",
    "        x = self.cnv3(x)\n",
    "        x = self.iact(self.inrm(torch.add(x, res)))\n",
    "        x = self.iact(self.uscale(x))\n",
    "        return x\n",
    "\n",
    "class FeatConv(nn.Module):\n",
    "\n",
    "    def __init__(self, fsize):\n",
    "        super(FeatConv, self).__init__()\n",
    "        self.cnv1 = nn.Conv2d(1, fsize, (3, 41), 1, 1, bias=False)\n",
    "        self.cnv2 = nn.Conv2d(fsize, fsize, (3, 41), 1, 1, bias=False)\n",
    "        self.cnv3 = nn.Conv2d(fsize, fsize, (3, 41), 1, 1, bias=False)\n",
    "        self.nrm1 = nn.BatchNorm2d(fsize)\n",
    "        self.nrm2 = nn.BatchNorm2d(fsize)\n",
    "        self.nrm3 = nn.BatchNorm2d(fsize)\n",
    "        self.fpool = nn.AdaptiveAvgPool2d(4)\n",
    "        self.act = nn.ELU()\n",
    "        self.flat = nn.Flatten()\n",
    "        self.drop = nn.Dropout2d(0.16)\n",
    "        self.ll1 = nn.Linear(512, 128)\n",
    "        self.ll2 = nn.Linear(128, 64)\n",
    "        self.ll3 = nn.Linear(64, 32)\n",
    "        self.ll4 = nn.Linear(32, 32)\n",
    "        self.ll5 = nn.Linear(32, 64)\n",
    "        self.ll6 = nn.Linear(64, 128)\n",
    "        self.ll7 = nn.Linear(128, 576)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.nrm1(self.cnv1(x)))\n",
    "        x = self.act(self.nrm2(self.cnv2(x)))\n",
    "        x = self.act(self.nrm3(self.cnv3(x)))\n",
    "        x = self.drop(x)\n",
    "        x = self.fpool(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.act(self.ll1(x))\n",
    "        x = self.act(self.ll2(x))\n",
    "        x = self.act(self.ll3(x))\n",
    "        x = self.act(self.ll4(x))\n",
    "        x = self.act(self.ll5(x))\n",
    "        x = self.act(self.ll6(x))\n",
    "        x = self.act(self.ll7(x))\n",
    "        return x\n",
    "\n",
    "class MusCls(nn.Module):\n",
    "\n",
    "    def __init__(self, step_1_ftrs: int, step_2_ftrs: int, step_3_ftrs: int, step_4_ftrs: int):\n",
    "        #torch.manual_seed(1024)\n",
    "        torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "        super(MusCls, self).__init__()\n",
    "        self.final_ftrs = step_4_ftrs // 2\n",
    "        self.ecnv1 = nn.Conv2d(1, step_1_ftrs, 3, 1, 1)\n",
    "        self.ecnv2 = nn.Conv2d(step_1_ftrs, step_1_ftrs, 3, 1, 1)\n",
    "        self.ecnv3 = nn.Conv2d(step_1_ftrs, step_1_ftrs, 3, 1, 1)\n",
    "        self.ftrs = FeatConv(step_1_ftrs)\n",
    "        self.fcnv1 = nn.Conv2d(step_4_ftrs, self.final_ftrs, 3, 1, 1, bias=False)\n",
    "        self.fcnv2 = nn.Conv2d(step_4_ftrs, self.final_ftrs, 3, 1, 1, bias=False)\n",
    "        self.fcnv3 = nn.Conv2d(step_4_ftrs, self.final_ftrs, 3, 1, 1, bias=False)\n",
    "        self.nrm1 = nn.BatchNorm2d(self.final_ftrs)\n",
    "        self.nrm2 = nn.BatchNorm2d(self.final_ftrs)\n",
    "        self.nrm3 = nn.BatchNorm2d(self.final_ftrs)\n",
    "        self.lstep1 = ReStep(step_1_ftrs, (1, 2))\n",
    "        self.lstep2 = ReStep(step_2_ftrs, (1, 2))\n",
    "        self.lstep3 = ReStep(step_3_ftrs, 2, True)\n",
    "        self.lstep4 = ReStep(step_4_ftrs, 2, True)\n",
    "        self.mstep1 = ReStep(step_1_ftrs, (1, 2))\n",
    "        self.mstep2 = ReStep(step_2_ftrs, 2)\n",
    "        self.mstep3 = ReStep(step_3_ftrs, 2, True)\n",
    "        self.mstep4 = ReStep(step_4_ftrs, 2, True)\n",
    "        self.hstep1 = ReStep(step_1_ftrs, 2)\n",
    "        self.hstep2 = ReStep(step_2_ftrs, 2)\n",
    "        self.hstep3 = ReStep(step_3_ftrs, 2, True)\n",
    "        self.hstep4 = ReStep(step_4_ftrs, 2, True)\n",
    "        self.flin = nn.Linear(self.final_ftrs * 9 * 4, self.final_ftrs)\n",
    "        self.flin2 = nn.Linear(self.final_ftrs, 19)\n",
    "        self.act = nn.ELU()\n",
    "        self.flat = nn.Flatten()\n",
    "        self.fmxpool = nn.AdaptiveAvgPool2d(3)\n",
    "        self.drop = nn.Dropout2d(0.12)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = inp[0]\n",
    "        y = inp[1]\n",
    "        y = self.ftrs(y)\n",
    "        x = self.act(self.ecnv1(x))\n",
    "        x = self.act(self.ecnv2(x))\n",
    "        x = self.act(self.ecnv3(x))\n",
    "        x = self.drop(x)\n",
    "        lows = x[:,:, :16, :]\n",
    "        mids = x[:,:, 14:62, :]\n",
    "        highs = x[:,:, 60:, :]\n",
    "        for i, step in enumerate([\n",
    "                (self.lstep1, self.mstep1, self.hstep1),\n",
    "                (self.lstep2, self.mstep2, self.hstep2),\n",
    "                (self.lstep3, self.mstep3, self.hstep3),\n",
    "                (self.lstep4, self.mstep4, self.hstep4)\n",
    "            ]):\n",
    "            lows = step[0](lows)\n",
    "            mids = step[1](mids)\n",
    "            highs = step[2](highs)\n",
    "            if i >= 2:\n",
    "                lows = self.drop(lows)\n",
    "                mids = self.drop(mids)\n",
    "                highs = self.drop(highs)\n",
    "        lows = self.act(self.nrm1(self.fcnv1(lows)))\n",
    "        mids = self.act(self.nrm2(self.fcnv2(mids)))\n",
    "        highs = self.act(self.nrm3(self.fcnv3(highs)))\n",
    "        lows = self.flat(self.fmxpool(lows))\n",
    "        mids = self.flat(self.fmxpool(mids))\n",
    "        highs = self.flat(self.fmxpool(highs))\n",
    "        x = torch.cat((y, lows, mids, highs), 1)\n",
    "        x = self.flin2(self.act(self.flin(x)))\n",
    "        return x\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model: nn.Module, model_name: str, lrn_rate1: float, lrn_rate2: float, wdecay1: float, wdecay2: float, save_threshold: float) -> None:\n",
    "        self.model = model.to('cuda')\n",
    "        self.save_threshold = save_threshold\n",
    "        self.lrn_rate1 = lrn_rate1\n",
    "        self.lrn_rate2 = lrn_rate2\n",
    "        self.wdecay1 = wdecay1\n",
    "        self.wdecay2 = wdecay2\n",
    "        self.optimizer = torch.optim.NAdam(self.model.parameters(), lr=lrn_rate1, weight_decay=wdecay1)\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "        self.scaler = GradScaler(enabled=True)\n",
    "        self.model_name = model_name\n",
    "        self.tblog = SummaryWriter(log_dir=f'C:/Users/BBA/Coding/tblogs/{proj_name}/{model_name}')\n",
    "        self.tbatch_counter = 0\n",
    "        self.vbatch_counter = 0\n",
    "\n",
    "    def split_data(self, dataset: list[tuple[torch.Tensor, int]], ratio: float, splits: int) -> tuple[list[list[tuple[torch.Tensor, int]]], list[tuple[torch.Tensor, int]]]:\n",
    "        vdata = []\n",
    "        counts = collections.Counter([x[1] for x in dataset])\n",
    "        for x in counts.items():\n",
    "            gcounter = 0\n",
    "            i = 0\n",
    "            while gcounter < int(x[1] * ratio):\n",
    "                if dataset[i][1] == x[0]:\n",
    "                    vdata.append(dataset.pop(i))\n",
    "                    gcounter += 1\n",
    "                i += 1\n",
    "        tdata = [dataset[(x - 1) * int(len(dataset) / splits) : x * int(len(dataset) / splits)] for x in range(1, splits + 1)]\n",
    "        return tdata, vdata\n",
    "\n",
    "    def _train(self, dataset: list[tuple[torch.Tensor, int]]):\n",
    "        total_loss = 0\n",
    "        self.model.train()\n",
    "        for i, x in enumerate(dataset):\n",
    "            with autocast(enabled=True):\n",
    "                dat1, dat2, labels = x[0][0].to('cuda'), x[0][1].to('cuda'), x[1].to('cuda')\n",
    "                results = self.model((dat1, dat2))\n",
    "                loss = self.loss_function(results, labels)\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "            total_loss += loss.item()\n",
    "            self.optimizer.zero_grad()\n",
    "            self.tblog.add_scalar('Train Loss / Batch', total_loss / (i + 1), self.tbatch_counter)\n",
    "            self.tbatch_counter += 1\n",
    "        return total_loss / len(dataset)\n",
    "\n",
    "    def _validate(self, dataset: list[tuple[torch.Tensor, int]]):\n",
    "        total_loss = 0\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for i, x in enumerate(dataset):\n",
    "                dat1, dat2, labels = x[0][0].to('cuda'), x[0][1].to('cuda'), x[1].to('cuda')\n",
    "                results = self.model((dat1, dat2))\n",
    "                for j, w in enumerate([F.softmax(y, dim=0).argmax() for y in results]):\n",
    "                    predictions.append((w, labels[j]))\n",
    "                total_loss += self.loss_function(results, labels).item()\n",
    "                self.tblog.add_scalar('Validation Loss / Batch', total_loss / (i + 1), self.vbatch_counter)\n",
    "                self.vbatch_counter += 1\n",
    "        return total_loss / len(dataset), predictions\n",
    "\n",
    "    def train_model(self, dataset: list[tuple[torch.Tensor, int]], target_epochs: int, batch_size: int, ratio: float, splits: int, aug_epochs: list[int]):\n",
    "        self.batch_size = batch_size\n",
    "        samples = len(dataset)\n",
    "        train_data, val_data = self.split_data(dataset, ratio, splits)\n",
    "        \n",
    "        for ep in range(target_epochs):\n",
    "            for i, group in enumerate(train_data):\n",
    "                self.batches_per_group = int(int(samples * (1 - ratio)) / splits / self.batch_size)\n",
    "\n",
    "\n",
    "                if ep in aug_epochs:\n",
    "                    for x in self.optimizer.param_groups:\n",
    "                        x['lr'] = self.lrn_rate2\n",
    "                        x['weight_decay'] = self.wdecay2\n",
    "                    aug_data = aug_imgs(group.copy())\n",
    "                    tdata = DataLoader(aug_data, batch_size=self.batch_size, shuffle=True, generator=torch.Generator(device='cuda'))\n",
    "                else:\n",
    "                    for x in self.optimizer.param_groups:\n",
    "                        x['lr'] = self.lrn_rate1\n",
    "                        x['weight_decay'] = self.wdecay1\n",
    "                    tdata = DataLoader(group, batch_size=self.batch_size, shuffle=True, generator=torch.Generator(device='cuda'))\n",
    "                train_loss = self._train(tdata)\n",
    "                vdata = DataLoader(val_data, batch_size=self.batch_size, shuffle=True, generator=torch.Generator(device='cuda'))\n",
    "                val_loss, predictions = self._validate(vdata)\n",
    "\n",
    "\n",
    "                print(f'Status at batch {self.batches_per_group * (1 + i)},\\tTLoss: {round(float(train_loss), 5)}\\tVLoss: {round(float(val_loss), 5)}')\n",
    "                mac_prec, mac_rec, f1_mac, f1_mic, mcc, ck = print_metrics(predictions)\n",
    "                self.tblog.add_scalar('Macro Avg Precision / Epoch', mac_prec / (ep + 1), ep + 1)\n",
    "                self.tblog.add_scalar('Macro Avg Recall / Epoch', mac_rec / (ep + 1), ep + 1)\n",
    "                self.tblog.add_scalar('F1 Macro / Epoch', f1_mac / (ep + 1), ep + 1)\n",
    "                self.tblog.add_scalar('F1 Micro / Epoch', f1_mic / (ep + 1), ep + 1)\n",
    "                self.tblog.add_scalar('Mattheus Correlation Coefficient / Epoch', mcc / (ep + 1), ep + 1)\n",
    "                self.tblog.add_scalar('Cohens Kappa / Epoch', ck / (ep + 1), ep + 1)\n",
    "\n",
    "\n",
    "                if val_loss < self.save_threshold:\n",
    "                    self.save_threshold = val_loss\n",
    "                    torch.save(self.model.state_dict(), f'models/{self.model_name}')\n",
    "\n",
    "                    \n",
    "        self.model = self.model.to('cpu')\n",
    "        del dataset\n",
    "        self.tblog.close()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def group_predict(adata: list[list[np.ndarray]], model: nn.Module) -> list[tuple[int, int]]:\n",
    "    predictions = []\n",
    "    model.to('cuda')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for song in adata:\n",
    "            results = []\n",
    "            for frame in song[0]:\n",
    "                spctr, ftr = frame[0].to('cuda'), frame[1].to('cuda')\n",
    "                results.append(torch.nn.functional.softmax(*model((spctr, ftr)), dim=0).tolist())\n",
    "            scores = [round(sum(w), 4) for w in  np.array(results).T]\n",
    "            predictions.append((song[1], scores.index(np.max(scores))))\n",
    "    model.to('cpu')\n",
    "    return predictions\n",
    "\n",
    "def aug_imgs(img_data: list[tuple[torch.Tensor, int]]) -> list[tuple[torch.Tensor, int]]:\n",
    "    aug_count = int(len(img_data) * 0.3678)\n",
    "    change = img_data[:aug_count]\n",
    "    final = img_data[aug_count:]\n",
    "    change = [((x[0][0] + 55).to(torch.uint8), x[0][1], x[1]) for x in change]\n",
    "    new_pack = []\n",
    "    for item in change:\n",
    "        label = item[1], item[2]\n",
    "        img = np.random.choice(transforms)(item[0])\n",
    "        if img.size() != (140, 128):\n",
    "            img = torchvision.transforms.functional.resize(img, (140, 128))\n",
    "        new_pack.append((img, label))\n",
    "    new_pack = [((x[0].to(torch.float32) - 55, x[1][0]), x[1][1]) for x in new_pack]\n",
    "    final.extend(new_pack)\n",
    "    return final\n",
    "\n",
    "def load_test(filename, vmode=False):\n",
    "    with open(f'data/{filename}', 'rb') as f:\n",
    "        adata = pickle.load(f)\n",
    "    with open('data/train_labels', 'rt') as f:\n",
    "        labels = f.readlines()\n",
    "    labels = [x.split(',') for x in labels]\n",
    "    labels = {x[1]: int(x[3]) for x in labels}\n",
    "    temp = []\n",
    "    for x in adata:\n",
    "        if vmode:\n",
    "            temp.append(([(torch.tensor(y, dtype=torch.float32).unsqueeze(0).unsqueeze(0), torch.tensor(x[1], dtype=torch.float32).unsqueeze(0).unsqueeze(0)) for y in x[0]], labels[x[2]]))\n",
    "        else:\n",
    "            temp.append(([(torch.tensor(y, dtype=torch.float32).unsqueeze(0).unsqueeze(0), torch.tensor(x[1], dtype=torch.float32).unsqueeze(0).unsqueeze(0)) for y in x[0]], x[2]))\n",
    "    return temp\n",
    "\n",
    "def load_train(filename):\n",
    "    with open('data/train_labels', 'rt') as f:\n",
    "        labels = f.readlines()\n",
    "    labels = [x.split(',') for x in labels]\n",
    "    labels = {x[1]: int(x[3]) for x in labels}\n",
    "    with open(f'data/{filename}', 'rb') as f:\n",
    "        adata = pickle.load(f)\n",
    "    temp = []\n",
    "    for x in adata:\n",
    "        temp.extend([((torch.tensor(y, dtype=torch.float32).unsqueeze(0), torch.tensor(x[1], dtype=torch.float32).unsqueeze(0)), labels[x[2]]) for y in x[0]])\n",
    "    return temp\n",
    "\n",
    "def print_metrics(predictions):\n",
    "    print(f'Label\\t\\tRecall\\t\\tPrecision\\tF1')\n",
    "    metrics = []\n",
    "    for x in range(19):\n",
    "        gcount = 0\n",
    "        guesses = [y for y in predictions if y[1] == x]\n",
    "        actual = [y for y in predictions if y[0] == x]\n",
    "        for z in actual:\n",
    "            if z[0] == z[1]:\n",
    "                gcount += 1\n",
    "        metrics.append((len(actual), len(guesses), gcount))\n",
    "        print(f'{x}\\t\\t{round(gcount / len(actual), 3)}\\t\\t{round(gcount / len(guesses), 3)}\\t\\t{2*(((round(gcount / len(guesses), 3))*(round(gcount / len(actual), 3)))/((round(gcount / len(guesses), 3))+(round(gcount / len(actual), 3))))}')\n",
    "\n",
    "    total = len(predictions)\n",
    "    correct = sum([x[2] for x in metrics])\n",
    "    mprec = round(np.mean([x[2] / x[1] for x in metrics]), 4)\n",
    "    mrec = round(np.mean([x[2] / x[0] for x in metrics]), 4)\n",
    "    f1mac = round(2 * ((mprec * mrec) / (mprec**-1 + mrec**-1)), 4)\n",
    "    f1mic = round(correct / total, 4)\n",
    "    mcc = round((correct * total - sum([x[0] * x[1] for x in metrics])) / np.sqrt((total**2 - sum(x[0]**2 for x in metrics))*(total**2 - sum(x[1]**2 for x in metrics))), 4)\n",
    "    ck = round((correct * total - sum([x[1] * x[0] for x in metrics])) / (total**2 - sum([x[1] * x[0] for x in metrics])), 3)\n",
    "    print('\\n')\n",
    "    print(f\"Macro Precision\\t\\t{mprec}\")\n",
    "    print(f'Macro Recall\\t\\t{mrec}')\n",
    "    print(f'F1 Macro\\t\\t{f1mac}')\n",
    "    print(f'F1 Micro\\t\\t{f1mic}')\n",
    "    print(f'MCC\\t\\t\\t{mcc}')\n",
    "    print(f'CK\\t\\t\\t{ck}')\n",
    "    print(correct, '/', total)\n",
    "    return mprec, mrec, f1mac, f1mic, mcc, ck\n",
    "\n",
    "blur = torchvision.transforms.GaussianBlur(3, sigma=(0.1, 2.0))\n",
    "erase = torchvision.transforms.RandomErasing(p=0.5, scale=(0.02, 0.1), ratio=(0.8, 1.2), value=0, inplace=False)\n",
    "crop = torchvision.transforms.RandomResizedCrop((140, 128), scale=(0.84, 1.0))\n",
    "affine = torchvision.transforms.RandomAffine(0, translate=(0.2, 0), scale=(0.86, 1.14), shear=None, fill=0)\n",
    "invert = torchvision.transforms.RandomInvert()\n",
    "transforms = [blur, erase, crop, affine, invert]\n",
    "proj_name = 'Music_Classifier'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data = load_train('f3even')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2169251\n",
      "Status at batch 4241,\tTLoss: 1.66317\tTPAccuracy 18.95367%\tVLoss: 1.56698\tVGAccuracy 46.94196%\tVPAccuracy 20.86734%\n",
      "Status at batch 4241,\tTLoss: 1.45418\tTPAccuracy 23.35914%\tVLoss: 1.50979\tVGAccuracy 49.00298%\tVPAccuracy 22.09575%\n",
      "Status at batch 4241,\tTLoss: 1.41957\tTPAccuracy 24.18168%\tVLoss: 1.50163\tVGAccuracy 50.66964%\tVPAccuracy 22.27676%\n"
     ]
    }
   ],
   "source": [
    "train_session = 1\n",
    "av_model = MusCls(32, 64, 128, 128)\n",
    "print(sum(x.numel() for x in av_model.parameters() if x.requires_grad))\n",
    "\n",
    "#av_model.load_state_dict(torch.load('models/mscls_x'))\n",
    "trainer = Trainer(av_model, f'mscls_6_{train_session}', 3e-4, 2e-4, 1e-5, 2e-5, 1.3)\n",
    "trainer.train_model(audio_data, 3, 32, 0.09, 1, [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(av_model.state_dict(), 'models/mscls_6_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del av_model\n",
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_model = MusCls(32, 64, 128, 128)\n",
    "av_model.load_state_dict(torch.load('models/mscls_6_0'))\n",
    "test_data = load_test('f3eval', True)\n",
    "predictions = group_predict(test_data, av_model)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([x[2] / x[1] for x in metrics])\n",
    "np.mean([x[2] / x[0] for x in metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#av_model = MusCls(32, 64, 128, 128)\n",
    "#av_model.load_state_dict(torch.load('models/mscls_5_1'))\n",
    "test_data = load_test('f3test', False)\n",
    "predictions = group_predict(test_data, av_model)\n",
    "savename = 'f3'\n",
    "\n",
    "with open(f'predictions/{savename}.csv', 'wt') as f:\n",
    "    f.writelines(['song_id,genre_id\\n'])\n",
    "    f.writelines([f'{x[0].lstrip(\"0\").rstrip(\".ogg\")},{x[1]}\\n' for x in predictions])\n",
    "    f.writelines(['24013,0\\n', '22612,1\\n'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "489c9a9a56e86705d77b3b0d6716b8bf16bea0f07ee042748988e2e93c15e7dd"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
